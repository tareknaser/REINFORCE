{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "V8oadoJSWp7C"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "from collections import deque\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "# PyTorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical\n",
        "\n",
        "# Gym\n",
        "import gym\n",
        "import gym_pygame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "POOOk15_K6KA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "23e626be-d960-4152-a9ed-5dd95b15e688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.9/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ],
      "source": [
        "env_id = \"CartPole-v1\"\n",
        "# Create the env\n",
        "env = gym.make(env_id)\n",
        "\n",
        "# Create the evaluation env\n",
        "eval_env = gym.make(env_id)\n",
        "\n",
        "# Get the state space and action space\n",
        "s_size = env.observation_space.shape[0]\n",
        "a_size = env.action_space.n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "FMLFrjiBNLYJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9d2c858-5c01-4570-a8df-8aae4c5cf65a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "_____OBSERVATION SPACE_____ \n",
            "\n",
            "The State Space is:  4\n",
            "Sample observation [-3.6548412e+00  1.5944771e+38 -3.8751465e-01 -1.8501060e+38]\n"
          ]
        }
      ],
      "source": [
        "print(\"_____OBSERVATION SPACE_____ \\n\")\n",
        "print(\"The State Space is: \", s_size)\n",
        "print(\"Sample observation\", env.observation_space.sample()) # Get a random observation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Lu6t4sRNNWkN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04fc7d44-013c-4af2-c551-0ebd03f8ff64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " _____ACTION SPACE_____ \n",
            "\n",
            "The Action Space is:  2\n",
            "Action Space Sample 1\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n _____ACTION SPACE_____ \\n\")\n",
        "print(\"The Action Space is: \", a_size)\n",
        "print(\"Action Space Sample\", env.action_space.sample()) # Take a random action"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "Ho_UHf49N9i4"
      },
      "outputs": [],
      "source": [
        "class Policy(nn.Module):\n",
        "    def __init__(self, s_size, a_size, h_size):\n",
        "        super(Policy, self).__init__()\n",
        "        self.fc1 = nn.Linear(s_size, h_size)\n",
        "        self.fc2 = nn.Linear(h_size, a_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.softmax(x, dim=1)\n",
        "    \n",
        "    def act(self, state):\n",
        "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
        "        probs = self.forward(state).cpu()\n",
        "        m = Categorical(probs)\n",
        "        action = m.sample()\n",
        "        return action.item(), m.log_prob(action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "NCNvyElRStWG"
      },
      "outputs": [],
      "source": [
        "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
        "    scores_deque = deque(maxlen=100)\n",
        "    scores = []\n",
        "    for i_episode in range(1, n_training_episodes+1):\n",
        "        saved_log_probs = []\n",
        "        rewards = []\n",
        "        state = env.reset()\n",
        "        for t in range(max_t):\n",
        "            action, log_prob = policy.act(state)\n",
        "            saved_log_probs.append(log_prob)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "            rewards.append(reward)\n",
        "            if done:\n",
        "                break \n",
        "        scores_deque.append(sum(rewards))\n",
        "        scores.append(sum(rewards))\n",
        "        \n",
        "        returns = deque(maxlen=max_t) \n",
        "        n_steps = len(rewards) \n",
        "        # Compute the discounted returns at each timestep,\n",
        "        # as \n",
        "        #      the sum of the gamma-discounted return at time t (G_t) + the reward at time t\n",
        "        \n",
        "        ## Given the above, we calculate the returns at timestep t as: \n",
        "        #               gamma[t] * return[t] + reward[t]\n",
        "\n",
        "        for t in range(n_steps)[::-1]:\n",
        "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
        "            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n",
        "            \n",
        "        ## standardization of the returns is employed to make training more stable\n",
        "        eps = np.finfo(np.float32).eps.item()\n",
        "        returns = torch.tensor(returns)\n",
        "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
        "        \n",
        "        policy_loss = []\n",
        "        for log_prob, disc_return in zip(saved_log_probs, returns):\n",
        "            policy_loss.append(-log_prob * disc_return)\n",
        "        policy_loss = torch.cat(policy_loss).sum()\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        policy_loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        if i_episode % print_every == 0:\n",
        "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
        "        \n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "utRe1NgtVBYF"
      },
      "outputs": [],
      "source": [
        "cartpole_hyperparameters = {\n",
        "    \"h_size\": 16,\n",
        "    \"n_training_episodes\": 5000,\n",
        "    \"n_evaluation_episodes\": 10,\n",
        "    \"max_t\": 1000,\n",
        "    \"gamma\": 1.0,\n",
        "    \"lr\": 1e-2,\n",
        "    \"env_id\": env_id,\n",
        "    \"state_space\": s_size,\n",
        "    \"action_space\": a_size,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "metadata": {
        "id": "gawDGrm40fv1"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "D3lWyVXBVfl6"
      },
      "outputs": [],
      "source": [
        "cartpole_policy = Policy(cartpole_hyperparameters[\"state_space\"], cartpole_hyperparameters[\"action_space\"], cartpole_hyperparameters[\"h_size\"]).to(device)\n",
        "cartpole_optimizer = optim.Adam(cartpole_policy.parameters(), lr=cartpole_hyperparameters[\"lr\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "uGf-hQCnfouB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9abe6ba0-5723-4fac-9037-e864e217f98e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 100\tAverage Score: 44.28\n",
            "Episode 200\tAverage Score: 187.93\n",
            "Episode 300\tAverage Score: 292.66\n",
            "Episode 400\tAverage Score: 479.45\n",
            "Episode 500\tAverage Score: 494.56\n",
            "Episode 600\tAverage Score: 496.34\n",
            "Episode 700\tAverage Score: 500.00\n",
            "Episode 800\tAverage Score: 497.93\n",
            "Episode 900\tAverage Score: 444.88\n",
            "Episode 1000\tAverage Score: 455.46\n",
            "Episode 1100\tAverage Score: 444.60\n",
            "Episode 1200\tAverage Score: 497.00\n",
            "Episode 1300\tAverage Score: 481.89\n",
            "Episode 1400\tAverage Score: 479.62\n",
            "Episode 1500\tAverage Score: 500.00\n",
            "Episode 1600\tAverage Score: 418.59\n",
            "Episode 1700\tAverage Score: 485.48\n",
            "Episode 1800\tAverage Score: 331.90\n",
            "Episode 1900\tAverage Score: 244.59\n",
            "Episode 2000\tAverage Score: 127.68\n",
            "Episode 2100\tAverage Score: 362.16\n",
            "Episode 2200\tAverage Score: 500.00\n",
            "Episode 2300\tAverage Score: 500.00\n",
            "Episode 2400\tAverage Score: 500.00\n",
            "Episode 2500\tAverage Score: 500.00\n",
            "Episode 2600\tAverage Score: 500.00\n",
            "Episode 2700\tAverage Score: 500.00\n",
            "Episode 2800\tAverage Score: 500.00\n",
            "Episode 2900\tAverage Score: 500.00\n",
            "Episode 3000\tAverage Score: 500.00\n",
            "Episode 3100\tAverage Score: 500.00\n",
            "Episode 3200\tAverage Score: 341.21\n",
            "Episode 3300\tAverage Score: 105.70\n",
            "Episode 3400\tAverage Score: 86.39\n",
            "Episode 3500\tAverage Score: 114.07\n",
            "Episode 3600\tAverage Score: 122.39\n",
            "Episode 3700\tAverage Score: 119.03\n",
            "Episode 3800\tAverage Score: 95.26\n",
            "Episode 3900\tAverage Score: 105.44\n",
            "Episode 4000\tAverage Score: 135.44\n",
            "Episode 4100\tAverage Score: 234.85\n",
            "Episode 4200\tAverage Score: 476.65\n",
            "Episode 4300\tAverage Score: 394.36\n",
            "Episode 4400\tAverage Score: 500.00\n",
            "Episode 4500\tAverage Score: 195.21\n",
            "Episode 4600\tAverage Score: 153.77\n",
            "Episode 4700\tAverage Score: 434.81\n",
            "Episode 4800\tAverage Score: 433.74\n",
            "Episode 4900\tAverage Score: 485.73\n",
            "Episode 5000\tAverage Score: 500.00\n"
          ]
        }
      ],
      "source": [
        "scores = reinforce(cartpole_policy,\n",
        "                   cartpole_optimizer,\n",
        "                   cartpole_hyperparameters[\"n_training_episodes\"], \n",
        "                   cartpole_hyperparameters[\"max_t\"],\n",
        "                   cartpole_hyperparameters[\"gamma\"], \n",
        "                   100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3FamHmxyhBEU"
      },
      "outputs": [],
      "source": [
        "def evaluate_agent(env, max_steps, n_eval_episodes, policy):\n",
        "  \"\"\"\n",
        "  Evaluate the agent for ``n_eval_episodes`` episodes and returns average reward and std of reward.\n",
        "  :param env: The evaluation environment\n",
        "  :param n_eval_episodes: Number of episode to evaluate the agent\n",
        "  :param policy: The Reinforce agent\n",
        "  \"\"\"\n",
        "  episode_rewards = []\n",
        "  for episode in range(n_eval_episodes):\n",
        "    state = env.reset()\n",
        "    step = 0\n",
        "    done = False\n",
        "    total_rewards_ep = 0\n",
        "    \n",
        "    for step in range(max_steps):\n",
        "      action, _ = policy.act(state)\n",
        "      new_state, reward, done, info = env.step(action)\n",
        "      total_rewards_ep += reward\n",
        "        \n",
        "      if done:\n",
        "        break\n",
        "      state = new_state\n",
        "    episode_rewards.append(total_rewards_ep)\n",
        "  mean_reward = np.mean(episode_rewards)\n",
        "  std_reward = np.std(episode_rewards)\n",
        "\n",
        "  return mean_reward, std_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "ohGSXDyHh0xx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d384695-5d7e-4301-917a-8e9face5f12a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(500.0, 0.0)"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "evaluate_agent(eval_env, \n",
        "               cartpole_hyperparameters[\"max_t\"], \n",
        "               cartpole_hyperparameters[\"n_evaluation_episodes\"],\n",
        "               cartpole_policy)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "BPLwsPajb1f8",
        "L_WSo0VUV99t",
        "mjY-eq3eWh9O",
        "JoTC9o2SczNn",
        "gfGJNZBUP7Vn",
        "YB0Cxrw1StrP",
        "47iuAFqV8Ws-",
        "x62pP0PHdA-y"
      ]
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}